\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{marcus2018deep}
\citation{sap2019risk}
\citation{das2020opportunities}
\citation{lundberg2017unified}
\citation{sundararajan2017axiomatic}
\citation{kapishnikov2021guided}
\citation{sundararajan2017axiomatic}
\newlabel{sec:introduction}{{1}{1}{}{section.1}{}}
\newlabel{sec:introduction@cref}{{[section][1][]1}{[1][1][]1}}
\newlabel{fig:puppy}{{1}{1}{Comparing attributions of Integrated Gradients (middle image) with Geodesic Integrated Gradient (right image). In this work we show how to generate shaper explanations by integrating gradients along the geodesic line, as opposed to the straight line}{figure.1}{}}
\newlabel{fig:puppy@cref}{{[figure][1][]1}{[1][1][]1}}
\newlabel{eq:igi}{{1}{1}{}{equation.1.1}{}}
\newlabel{eq:igi@cref}{{[equation][1][]1}{[1][1][]1}}
\citation{deyoung2019eraser}
\citation{shrikumar2017learning}
\citation{deyoung2019eraser}
\citation{shrikumar2017learning}
\newlabel{fig:ig}{{2}{2}{\textbf {Integrated Gradients attributions.} The colour map represents $\textrm {IG}_1(\textbf {x})$ for each point $\textbf {x}$, with $(-0.5, -0.5)$ (point B) as the baseline. Points around $A$ have much higher attributions than other points on the top moon, despite all being sufficiently above the decision boundary of the MLP. This is due to the path being close to the decision boundary, resulting in high gradients (gray arrows) along this path}{figure.2}{}}
\newlabel{fig:ig@cref}{{[figure][2][]2}{[1][2][]2}}
\newlabel{fig:voc_metrics}{{3}{2}{\textbf {Metrics comparison} Evaluation of our results via comprehensiveness \citep {deyoung2019eraser} and log odds \citep {shrikumar2017learning} show Geodesic Integrated Gradients significantly outperforming other methods}{figure.3}{}}
\newlabel{fig:voc_metrics@cref}{{[figure][3][]3}{[1][2][]2}}
\citation{pascal-voc-2012}
\citation{yang2018geodesic,chen2019fast}
\citation{sundararajan2017axiomatic}
\newlabel{sec:method}{{2}{3}{}{section.2}{}}
\newlabel{sec:method@cref}{{[section][2][]2}{[1][3][]3}}
\newlabel{eq:taylor}{{2}{3}{}{equation.2.2}{}}
\newlabel{eq:taylor@cref}{{[equation][2][]2}{[1][3][]3}}
\newlabel{eq:inner_product}{{3}{3}{}{equation.2.3}{}}
\newlabel{eq:inner_product@cref}{{[equation][3][]3}{[1][3][]3}}
\newlabel{eq:length}{{4}{3}{}{equation.2.4}{}}
\newlabel{eq:length@cref}{{[equation][4][]4}{[1][3][]3}}
\newlabel{rem:shortest}{{2.1}{3}{}{theorem.2.1}{}}
\newlabel{rem:shortest@cref}{{[remark][1][2]2.1}{[1][3][]3}}
\newlabel{eq:loc_geo}{{5}{3}{}{equation.2.5}{}}
\newlabel{eq:loc_geo@cref}{{[equation][5][]5}{[1][3][]3}}
\newlabel{eq:log_geo_approx}{{6}{3}{}{equation.2.6}{}}
\newlabel{eq:log_geo_approx@cref}{{[equation][6][]6}{[1][3][]3}}
\newlabel{fig:method}{{4}{4}{\textbf {Method overview.} For an input $\textbf {x}$, a baseline $\overline {\textbf {x}}$, and a set of points $\textbf {x}_i$, we compute the kNN graph using the euclidean distance (dashed lines). For each couple $(\textbf {x}_i, \textbf {x}_j)$, we then compute the integrated gradients $\textrm {L}^*_{ij}$ using Equation \ref {eq:log_geo_approx}. For clarity, not all $\textrm {L}^*_{ij}$ are present on the figure. 0 and 7 represent $\overline {\textbf {x}}$ and $\textbf {x}$ respectively. Using the resulting undirected weighted graph, we use the Dijkstra algorithm to find the shortest path between $\textbf {x}$ and $\overline {\textbf {x}}$ (blue continuous lines). On the left, the points $\textbf {x}_i$ are provided while, on the right, the points are generated along the straight line between $\textbf {x}$ and $\overline {\textbf {x}}$ (dotted line)}{figure.4}{}}
\newlabel{fig:method@cref}{{[figure][4][]4}{[1][4][]4}}
\newlabel{eq:geodesic_ig}{{7}{4}{}{equation.2.7}{}}
\newlabel{eq:geodesic_ig@cref}{{[equation][7][]7}{[1][4][]4}}
\newlabel{eq:deriv_lip}{{8}{4}{}{equation.2.8}{}}
\newlabel{eq:deriv_lip@cref}{{[equation][8][]8}{[1][4][]4}}
\newlabel{fig:bridge}{{5}{4}{When the kNN graph is disconnected, as illustrated here, it would be impossible to compute Geodesic IG between certain points, for instance $\textbf {x}_1$ and $\textbf {x}_5$ here. To solve this, we add a single link between disconnected graphs, here between $\textbf {x}_3$ and $\textbf {x}_7$}{figure.5}{}}
\newlabel{fig:bridge@cref}{{[figure][5][]5}{[1][4][]4}}
\citation{sundararajan2017axiomatic}
\citation{sundararajan2017axiomatic}
\newlabel{sec:experiments}{{2.3}{5}{}{equation.2.9}{}}
\newlabel{sec:experiments@cref}{{[subsection][3][2]2.3}{[1][5][]5}}
\newlabel{fig:svi_moons}{{6}{5}{\textbf {Visualising 10 random paths.} For a simple case of half-moons with 1000 samples, we show the sampled paths between 10 pairs of points. As we see on low gradient regions the sampler prefers straigh lines, whereas in high gredient regions the path becomes closer to perpendicular to the large gradient vectors to cross the region as quickly as possible}{figure.6}{}}
\newlabel{fig:svi_moons@cref}{{[figure][6][]6}{[1][5][]5}}
\citation{sundararajan2017axiomatic}
\citation{sundararajan2017axiomatic}
\citation{scikit-learn}
\citation{jha2020enhanced}
\citation{lundberg2017unified}
\citation{smilkov2017smoothgrad}
\newlabel{eq:norms}{{10}{6}{}{equation.2.10}{}}
\newlabel{eq:norms@cref}{{[equation][10][]10}{[1][6][]6}}
\newlabel{sec:experiments}{{3}{6}{}{section.3}{}}
\newlabel{sec:experiments@cref}{{[section][3][]3}{[1][6][]6}}
\newlabel{subsec:half-moons}{{3.1}{6}{}{subsection.3.1}{}}
\newlabel{subsec:half-moons@cref}{{[subsection][1][3]3.1}{[1][6][]6}}
\newlabel{eq:moons-purity}{{11}{6}{}{equation.3.11}{}}
\newlabel{eq:moons-purity@cref}{{[equation][11][]11}{[1][6][]6}}
\newlabel{eq:moons-std}{{12}{6}{}{equation.3.12}{}}
\newlabel{eq:moons-std@cref}{{[equation][12][]12}{[1][6][]6}}
\citation{pascal-voc-2012}
\citation{he2016deep}
\citation{shrikumar2016not}
\citation{ribeiro2016should}
\citation{lundberg2017unified}
\citation{zeiler2014visualizing}
\citation{tonekaboni2020went}
\citation{jha2020enhanced}
\citation{deyoung2019eraser}
\citation{deyoung2019eraser}
\citation{shrikumar2017learning}
\newlabel{fig:qualitative-comp}{{7}{7}{Comparison between Integrated Gradients and Geodesic IG on one image. The predicted class is ``motor-scooter". The left figure is the original image, while the other two are heatmaps of respectively the IG (center) and the Geodesic IG (right). We can see that our method seems to reduce the amount of noise and is able to provide a sharper heatmap, compared with the vanilla IG. More comparisons are presented in Appendix \ref {app:voc}}{figure.7}{}}
\newlabel{fig:qualitative-comp@cref}{{[figure][7][]7}{[1][7][]7}}
\newlabel{tab:results_moons_2}{{1}{7}{Evaluation of different attribution methods on a half-moons dataset with Gaussian noise $\mathcal {N}(0, 0.2)$. The results over 5 different seeds are averaged, with the corresponding standard deviation in brackets. We present in Appendix \ref {app:half-moons} more results with different amounts of Gaussian noise}{table.1}{}}
\newlabel{tab:results_moons_2@cref}{{[table][1][]1}{[1][6][]7}}
\newlabel{subsec:voc}{{3.2}{7}{}{subsection.3.2}{}}
\newlabel{subsec:voc@cref}{{[subsection][2][3]3.2}{[1][7][]7}}
\citation{crane2020survey}
\citation{tenenbaum2000global}
\citation{jha2020enhanced}
\citation{kapishnikov2021guided}
\newlabel{tab:results_voc}{{2}{8}{Evaluation of different attribution methods on 100 randomly sampled images from the Pascal VOC test set. The metrics are computed by removing or keeping the top 5\% most important features. More results are provided in Appendix \ref {app:voc}}{table.2}{}}
\newlabel{tab:results_voc@cref}{{[table][2][]2}{[1][8][]8}}
\newlabel{sec:related_work}{{4}{8}{}{section.4}{}}
\newlabel{sec:related_work@cref}{{[section][4][]4}{[1][8][]8}}
\newlabel{fig:enhanced_ig}{{8}{8}{\textbf {Enhanced IG attributions.} Enhanced IG computes a kNN algorithm, uses Dijsktra to find the shortest path between an input and a reference baseline, and computes gradients along this path. However, this method is model agnostic and can as a result cross a high gradients region, which is the case in this example, between the input A and the baseline B. Input A therefore has a high attribution which does not reflect the model's true behavior. In this example, the noise is $\mathcal {N}(0, 0.15)$}{figure.8}{}}
\newlabel{fig:enhanced_ig@cref}{{[figure][8][]8}{[1][8][]8}}
\citation{dombrowski2019explanations}
\citation{sundararajan2017axiomatic}
\citation{langley00}
\bibdata{bib}
\bibcite{chen2019fast}{{1}{2019}{{Chen et~al.}}{{Chen, Ferroni, Klushyn, Paraschos, Bayer, and Smagt}}}
\bibcite{crane2020survey}{{2}{2020}{{Crane et~al.}}{{Crane, Livesu, Puppo, and Qin}}}
\bibcite{das2020opportunities}{{3}{2020}{{Das \& Rad}}{{Das and Rad}}}
\bibcite{deyoung2019eraser}{{4}{2019}{{DeYoung et~al.}}{{DeYoung, Jain, Rajani, Lehman, Xiong, Socher, and Wallace}}}
\bibcite{dombrowski2019explanations}{{5}{2019}{{Dombrowski et~al.}}{{Dombrowski, Alber, Anders, Ackermann, M{\"u}ller, and Kessel}}}
\bibcite{pascal-voc-2012}{{6}{}{{Everingham et~al.}}{{Everingham, Van~Gool, Williams, Winn, and Zisserman}}}
\bibcite{hassija2024interpreting}{{7}{2024}{{Hassija et~al.}}{{Hassija, Chamola, Mahapatra, Singal, Goel, Huang, Scardapane, Spinelli, Mahmud, and Hussain}}}
\bibcite{he2016deep}{{8}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{jha2020enhanced}{{9}{2020}{{Jha et~al.}}{{Jha, K~Aicher, R~Gazzara, Singh, and Barash}}}
\bibcite{kapishnikov2021guided}{{10}{2021}{{Kapishnikov et~al.}}{{Kapishnikov, Venugopalan, Avci, Wedin, Terry, and Bolukbasi}}}
\bibcite{lundberg2017unified}{{11}{2017}{{Lundberg \& Lee}}{{Lundberg and Lee}}}
\bibcite{marcus2018deep}{{12}{2018}{{Marcus}}{{}}}
\newlabel{sec:discussion}{{5}{9}{}{section.5}{}}
\newlabel{sec:discussion@cref}{{[section][5][]5}{[1][9][]9}}
\bibcite{scikit-learn}{{13}{2011}{{Pedregosa et~al.}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay}}}
\bibcite{ribeiro2016should}{{14}{2016}{{Ribeiro et~al.}}{{Ribeiro, Singh, and Guestrin}}}
\bibcite{sap2019risk}{{15}{2019}{{Sap et~al.}}{{Sap, Card, Gabriel, Choi, and Smith}}}
\bibcite{shrikumar2016not}{{16}{2016}{{Shrikumar et~al.}}{{Shrikumar, Greenside, Shcherbina, and Kundaje}}}
\bibcite{shrikumar2017learning}{{17}{2017}{{Shrikumar et~al.}}{{Shrikumar, Greenside, and Kundaje}}}
\bibcite{smilkov2017smoothgrad}{{18}{2017}{{Smilkov et~al.}}{{Smilkov, Thorat, Kim, Vi{\'e}gas, and Wattenberg}}}
\bibcite{sundararajan2017axiomatic}{{19}{2017}{{Sundararajan et~al.}}{{Sundararajan, Taly, and Yan}}}
\bibcite{tenenbaum2000global}{{20}{2000}{{Tenenbaum et~al.}}{{Tenenbaum, Silva, and Langford}}}
\bibcite{tonekaboni2020went}{{21}{2020}{{Tonekaboni et~al.}}{{Tonekaboni, Joshi, Campbell, Duvenaud, and Goldenberg}}}
\bibcite{yang2018geodesic}{{22}{2018}{{Yang et~al.}}{{Yang, Arvanitidis, Fu, Li, and Hauberg}}}
\bibcite{zeiler2014visualizing}{{23}{2014}{{Zeiler \& Fergus}}{{Zeiler and Fergus}}}
\bibstyle{icml2023}
\citation{dombrowski2019explanations}
\citation{dombrowski2019explanations}
\newlabel{app:softplus}{{A}{11}{}{appendix.A}{}}
\newlabel{app:softplus@cref}{{[appendix][1][2147483647]A}{[1][11][]11}}
\newlabel{fig:softplus}{{9}{11}{Evaluation of different attribution methods on the half-moons dataset using a MLP with ReLU, Softplus($\beta = 5$) and Softplus($\beta = 2$) activations. Integrated Gradient improves significantly when using Softplus activations, while other method either improve marginally or perform similarly as when using ReLU activations}{figure.9}{}}
\newlabel{fig:softplus@cref}{{[figure][9][2147483647]9}{[1][11][]11}}
\newlabel{fig:softplus_acc}{{10}{11}{Accuracy of the MLP with ReLU, Softplus($\beta = 5$) and Softplus($\beta = 2$) activations. We see a sharp drop of accuracy when $\beta $ decreases. We also plot the predictions on the test set of the MLP with ReLU activations (middle plot) and Softplus($\beta = 2$) activations (right plot) to illustrate this drop of accuracy}{figure.10}{}}
\newlabel{fig:softplus_acc@cref}{{[figure][10][2147483647]10}{[1][11][]11}}
\newlabel{app:half-moons}{{B}{12}{}{appendix.B}{}}
\newlabel{app:half-moons@cref}{{[appendix][2][2147483647]B}{[1][12][]12}}
\newlabel{fig:noises}{{11}{12}{Evaluation of different attribution methods on the half-moons dataset with different amounts of noise: $\mathcal {N}(0, x)$ where $x$ is defined as the axis of each plot}{figure.11}{}}
\newlabel{fig:noises@cref}{{[figure][11][2147483647]11}{[1][12][]12}}
\newlabel{fig:knn}{{12}{12}{Evaluation of Geodesic IG and Enhanced IG for different values of k in the kNN algorithm. We can see that increasing this parameter harms Enhanced IG performance, while it does not seem to have a major effect on Geodesic IG performance}{figure.12}{}}
\newlabel{fig:knn@cref}{{[figure][12][2147483647]12}{[1][12][]12}}
\newlabel{app:voc}{{C}{13}{}{appendix.C}{}}
\newlabel{app:voc@cref}{{[appendix][3][2147483647]C}{[1][13][]13}}
\newlabel{fig:topk}{{13}{13}{Evaluation of several attribution methods on Pascal VOC 2012 dataset, using different values of top k\%}{figure.13}{}}
\newlabel{fig:topk@cref}{{[figure][13][2147483647]13}{[1][13][]13}}
\newlabel{fig:more_images}{{14}{14}{Heatmaps of Integrated Gradients (middle) and Geodesic IG (right) on 5 randomly chosen images from the test set of Pascal VOC 2012. We can see that Geodesic IG heatmaps are sharper compared with the original IG method}{figure.14}{}}
\newlabel{fig:more_images@cref}{{[figure][14][2147483647]14}{[1][13][]14}}
\gdef \@abspage@last{14}
