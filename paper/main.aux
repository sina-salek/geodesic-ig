\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{marcus2018deep}
\citation{sap2019risk}
\citation{das2020opportunities}
\citation{shrikumar2017learning}
\citation{bach2015pixel}
\citation{ribeiro2016should}
\citation{holzinger2022explainable}
\citation{sundararajan2017axiomatic}
\newlabel{sec:introduction}{{1}{1}{}{section.1}{}}
\newlabel{sec:introduction@cref}{{[section][1][]1}{[1][1][]1}}
\newlabel{eq:igi}{{1}{1}{}{equation.1.1}{}}
\newlabel{eq:igi@cref}{{[equation][1][]1}{[1][1][]1}}
\citation{sundararajan2017axiomatic}
\citation{pascal-voc-2012}
\newlabel{fig:half_moons}{{1}{2}{\textbf {Half moons dataset} with Gaussian noise $\mathcal {N}(0, 0.1)$. Predictions of an MLP are illustrated in blue and red colours. The gradient field of the model is represented by gray arrows. We can see that, as expected, the gradients are very high right on the decision boundary. However, they drop off to nearly zero everywhere else. The regions on each side of the decision boundary are shaded with different colours. The accuracy on the test set for this model is 99.9\%}{figure.1}{}}
\newlabel{fig:half_moons@cref}{{[figure][1][]1}{[1][1][]2}}
\newlabel{fig:ig}{{2}{2}{\textbf {Integrated Gradients attributions.} The colour map represents $\textrm {IG}_1(\textbf {x})$ for each point $\textbf {x}$, with $(-0.5, -0.5)$ (point B) as the baseline. Points around $A$ have much higher attributions than other points on the top moon, despite all being sufficiently above the decision boundary of the MLP. This is due to the path being close to the decision boundary, resulting in high gradients (gray arrows) along this path}{figure.2}{}}
\newlabel{fig:ig@cref}{{[figure][2][]2}{[1][2][]2}}
\citation{yang2018geodesic,chen2019fast}
\citation{sundararajan2017axiomatic}
\newlabel{fig:geodesic_ig}{{3}{3}{\textbf {Geodesic IG attributions.} Geodesic IG successfully avoids high gradients regions, and presents as a result attributions free of any artefacts. Moreover, unlike IG, there is no high variation of attributions between close points. We provide a rigorous comparison of Geodesic IG against various baselines in the Experiment section}{figure.3}{}}
\newlabel{fig:geodesic_ig@cref}{{[figure][3][]3}{[1][2][]3}}
\newlabel{sec:method}{{2}{3}{}{section.2}{}}
\newlabel{sec:method@cref}{{[section][2][]2}{[1][3][]3}}
\newlabel{eq:taylor}{{2}{3}{}{equation.2.2}{}}
\newlabel{eq:taylor@cref}{{[equation][2][]2}{[1][3][]3}}
\newlabel{eq:inner_product}{{3}{3}{}{equation.2.3}{}}
\newlabel{eq:inner_product@cref}{{[equation][3][]3}{[1][3][]3}}
\newlabel{eq:length}{{4}{3}{}{equation.2.4}{}}
\newlabel{eq:length@cref}{{[equation][4][]4}{[1][3][]3}}
\newlabel{eq:loc_geo}{{5}{3}{}{equation.2.5}{}}
\newlabel{eq:loc_geo@cref}{{[equation][5][]5}{[1][3][]3}}
\citation{sundararajan2017axiomatic}
\citation{sundararajan2017axiomatic}
\citation{sundararajan2017axiomatic}
\newlabel{eq:log_geo_approx}{{6}{4}{}{equation.2.6}{}}
\newlabel{eq:log_geo_approx@cref}{{[equation][6][]6}{[1][4][]4}}
\newlabel{eq:geodesic_ig}{{7}{4}{}{equation.2.7}{}}
\newlabel{eq:geodesic_ig@cref}{{[equation][7][]7}{[1][4][]4}}
\newlabel{eq:norms}{{8}{4}{}{equation.2.8}{}}
\newlabel{eq:norms@cref}{{[equation][8][]8}{[1][4][]4}}
\newlabel{eq:deriv_lip}{{9}{4}{}{equation.2.9}{}}
\newlabel{eq:deriv_lip@cref}{{[equation][9][]9}{[1][4][]4}}
\citation{scikit-learn}
\newlabel{fig:method}{{4}{5}{\textbf {Method overview.} For an input $\textbf {x}$, a baseline $\overline {\textbf {x}}$, and a set of points $\textbf {x}_i$, we compute the kNN graph using the euclidean distance (dashed lines). For each couple $(\textbf {x}_i, \textbf {x}_j)$, we then compute the integrated gradients $\textrm {L}^*_{ij}$ using Equation \ref {eq:log_geo_approx}. For clarity, not all $\textrm {L}^*_{ij}$ are present on the figure. 0 and 7 represent $\overline {\textbf {x}}$ and $\textbf {x}$ respectively. Using the resulting undirected weighted graph, we use the Dijkstra algorithm to find the shortest path between $\textbf {x}$ and $\overline {\textbf {x}}$ (blue continuous lines). On the left, the points $\textbf {x}_i$ are provided while, on the right, the points are generated along the straight line between $\textbf {x}$ and $\overline {\textbf {x}}$ (dotted line)}{figure.4}{}}
\newlabel{fig:method@cref}{{[figure][4][]4}{[1][4][]5}}
\newlabel{eq:guide}{{10}{5}{}{equation.2.10}{}}
\newlabel{eq:guide@cref}{{[equation][10][]10}{[1][5][]5}}
\newlabel{fig:bridge}{{5}{5}{When the kNN graph is disconnected, as illustrated here, it would be impossible to compute Geodesic IG between certain points, for instance $\textbf {x}_1$ and $\textbf {x}_5$ here. To solve this, we add a single link between disconnected graphs, here between $\textbf {x}_3$ and $\textbf {x}_7$}{figure.5}{}}
\newlabel{fig:bridge@cref}{{[figure][5][]5}{[1][5][]5}}
\newlabel{sec:experiments}{{3}{5}{}{section.3}{}}
\newlabel{sec:experiments@cref}{{[section][3][]3}{[1][5][]5}}
\newlabel{subsec:half-moons}{{3.1}{5}{}{subsection.3.1}{}}
\newlabel{subsec:half-moons@cref}{{[subsection][1][3]3.1}{[1][5][]5}}
\citation{jha2020enhanced}
\citation{lundberg2017unified}
\citation{smilkov2017smoothgrad}
\citation{pascal-voc-2012}
\citation{he2016deep}
\citation{shrikumar2016not}
\citation{ribeiro2016should}
\citation{lundberg2017unified}
\citation{zeiler2014visualizing}
\citation{tonekaboni2020went}
\citation{jha2020enhanced}
\citation{deyoung2019eraser}
\citation{deyoung2019eraser}
\newlabel{eq:moons-purity}{{11}{6}{}{equation.3.11}{}}
\newlabel{eq:moons-purity@cref}{{[equation][11][]11}{[1][6][]6}}
\newlabel{eq:moons-std}{{12}{6}{}{equation.3.12}{}}
\newlabel{eq:moons-std@cref}{{[equation][12][]12}{[1][6][]6}}
\newlabel{tab:results_moons_2}{{1}{6}{Evaluation of different attribution methods on a half-moons dataset with Gaussian noise $\mathcal {N}(0, 0.2)$. The results over 5 different seeds are averaged, with the corresponding standard deviation in brackets. We present in Appendix \ref {app:half-moons} more results with different amounts of Gaussian noise}{table.1}{}}
\newlabel{tab:results_moons_2@cref}{{[table][1][]1}{[1][6][]6}}
\newlabel{subsec:voc}{{3.2}{6}{}{subsection.3.2}{}}
\newlabel{subsec:voc@cref}{{[subsection][2][3]3.2}{[1][6][]6}}
\citation{shrikumar2017learning}
\citation{crane2020survey}
\citation{tenenbaum2000global}
\citation{jha2020enhanced}
\newlabel{fig:qualitative-comp}{{6}{7}{Comparison between Integrated Gradients and Geodesic IG on one image. The predicted class is ``motor-scooter". The left figure is the original image, while the other two are heatmaps of respectively the IG (center) and the Geodesic IG (right). We can see that our method seems to reduce the amount of noise and is able to provide a sharper heatmap, compared with the vanilla IG. More comparisons are presented in Appendix \ref {app:voc}}{figure.6}{}}
\newlabel{fig:qualitative-comp@cref}{{[figure][6][]6}{[1][6][]7}}
\newlabel{tab:results_voc}{{2}{7}{Evaluation of different attribution methods on 100 randomly sampled images from the Pascal VOC test set. The metrics are computed by removing or keeping the top 5\% most important features. More results are provided in Appendix \ref {app:voc}}{table.2}{}}
\newlabel{tab:results_voc@cref}{{[table][2][]2}{[1][7][]7}}
\newlabel{sec:related_work}{{4}{7}{}{section.4}{}}
\newlabel{sec:related_work@cref}{{[section][4][]4}{[1][7][]7}}
\citation{kapishnikov2021guided}
\citation{dombrowski2019explanations}
\citation{sundararajan2017axiomatic}
\citation{langley00}
\bibdata{bib}
\bibcite{bach2015pixel}{{1}{2015}{{Bach et~al.}}{{Bach, Binder, Montavon, Klauschen, M{\"u}ller, and Samek}}}
\newlabel{fig:enhanced_ig}{{7}{8}{\textbf {Enhanced IG attributions.} Enhanced IG computes a kNN algorithm, uses Dijsktra to find the shortest path between an input and a reference baseline, and computes gradients along this path. However, this method is model agnostic and can as a result cross a high gradients region, which is the case in this example, between the input A and the baseline B. Input A therefore has a high attribution which does not reflect the model's true behavior. In this example, the noise is $\mathcal {N}(0, 0.15)$}{figure.7}{}}
\newlabel{fig:enhanced_ig@cref}{{[figure][7][]7}{[1][8][]8}}
\newlabel{sec:discussion}{{5}{8}{}{section.5}{}}
\newlabel{sec:discussion@cref}{{[section][5][]5}{[1][8][]8}}
\bibcite{chen2019fast}{{2}{2019}{{Chen et~al.}}{{Chen, Ferroni, Klushyn, Paraschos, Bayer, and Smagt}}}
\bibcite{crane2020survey}{{3}{2020}{{Crane et~al.}}{{Crane, Livesu, Puppo, and Qin}}}
\bibcite{das2020opportunities}{{4}{2020}{{Das \& Rad}}{{Das and Rad}}}
\bibcite{deyoung2019eraser}{{5}{2019}{{DeYoung et~al.}}{{DeYoung, Jain, Rajani, Lehman, Xiong, Socher, and Wallace}}}
\bibcite{dombrowski2019explanations}{{6}{2019}{{Dombrowski et~al.}}{{Dombrowski, Alber, Anders, Ackermann, M{\"u}ller, and Kessel}}}
\bibcite{pascal-voc-2012}{{7}{}{{Everingham et~al.}}{{Everingham, Van~Gool, Williams, Winn, and Zisserman}}}
\bibcite{he2016deep}{{8}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{holzinger2022explainable}{{9}{2022}{{Holzinger et~al.}}{{Holzinger, Saranti, Molnar, Biecek, and Samek}}}
\bibcite{jha2020enhanced}{{10}{2020}{{Jha et~al.}}{{Jha, K~Aicher, R~Gazzara, Singh, and Barash}}}
\bibcite{kapishnikov2021guided}{{11}{2021}{{Kapishnikov et~al.}}{{Kapishnikov, Venugopalan, Avci, Wedin, Terry, and Bolukbasi}}}
\bibcite{lundberg2017unified}{{12}{2017}{{Lundberg \& Lee}}{{Lundberg and Lee}}}
\bibcite{marcus2018deep}{{13}{2018}{{Marcus}}{{}}}
\bibcite{scikit-learn}{{14}{2011}{{Pedregosa et~al.}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay}}}
\bibcite{ribeiro2016should}{{15}{2016}{{Ribeiro et~al.}}{{Ribeiro, Singh, and Guestrin}}}
\bibcite{sap2019risk}{{16}{2019}{{Sap et~al.}}{{Sap, Card, Gabriel, Choi, and Smith}}}
\bibcite{shrikumar2016not}{{17}{2016}{{Shrikumar et~al.}}{{Shrikumar, Greenside, Shcherbina, and Kundaje}}}
\bibcite{shrikumar2017learning}{{18}{2017}{{Shrikumar et~al.}}{{Shrikumar, Greenside, and Kundaje}}}
\bibcite{smilkov2017smoothgrad}{{19}{2017}{{Smilkov et~al.}}{{Smilkov, Thorat, Kim, Vi{\'e}gas, and Wattenberg}}}
\bibcite{sundararajan2017axiomatic}{{20}{2017}{{Sundararajan et~al.}}{{Sundararajan, Taly, and Yan}}}
\bibcite{tenenbaum2000global}{{21}{2000}{{Tenenbaum et~al.}}{{Tenenbaum, Silva, and Langford}}}
\bibcite{tonekaboni2020went}{{22}{2020}{{Tonekaboni et~al.}}{{Tonekaboni, Joshi, Campbell, Duvenaud, and Goldenberg}}}
\bibcite{yang2018geodesic}{{23}{2018}{{Yang et~al.}}{{Yang, Arvanitidis, Fu, Li, and Hauberg}}}
\bibcite{zeiler2014visualizing}{{24}{2014}{{Zeiler \& Fergus}}{{Zeiler and Fergus}}}
\bibstyle{icml2023}
\citation{dombrowski2019explanations}
\citation{dombrowski2019explanations}
\newlabel{app:softplus}{{A}{10}{}{appendix.A}{}}
\newlabel{app:softplus@cref}{{[appendix][1][2147483647]A}{[1][10][]10}}
\newlabel{fig:softplus}{{8}{10}{Evaluation of different attribution methods on the half-moons dataset using a MLP with ReLU, Softplus($\beta = 5$) and Softplus($\beta = 2$) activations. Integrated Gradient improves significantly when using Softplus activations, while other method either improve marginally or perform similarly as when using ReLU activations}{figure.8}{}}
\newlabel{fig:softplus@cref}{{[figure][8][2147483647]8}{[1][10][]10}}
\newlabel{fig:softplus_acc}{{9}{10}{Accuracy of the MLP with ReLU, Softplus($\beta = 5$) and Softplus($\beta = 2$) activations. We see a sharp drop of accuracy when $\beta $ decreases. We also plot the predictions on the test set of the MLP with ReLU activations (middle plot) and Softplus($\beta = 2$) activations (right plot) to illustrate this drop of accuracy}{figure.9}{}}
\newlabel{fig:softplus_acc@cref}{{[figure][9][2147483647]9}{[1][10][]10}}
\newlabel{app:half-moons}{{B}{11}{}{appendix.B}{}}
\newlabel{app:half-moons@cref}{{[appendix][2][2147483647]B}{[1][11][]11}}
\newlabel{fig:noises}{{10}{11}{Evaluation of different attribution methods on the half-moons dataset with different amounts of noise: $\mathcal {N}(0, x)$ where $x$ is defined as the axis of each plot}{figure.10}{}}
\newlabel{fig:noises@cref}{{[figure][10][2147483647]10}{[1][11][]11}}
\newlabel{fig:knn}{{11}{11}{Evaluation of Geodesic IG and Enhanced IG for different values of k in the kNN algorithm. We can see that increasing this parameter harms Enhanced IG performance, while it does not seem to have a major effect on Geodesic IG performance}{figure.11}{}}
\newlabel{fig:knn@cref}{{[figure][11][2147483647]11}{[1][11][]11}}
\newlabel{app:voc}{{C}{12}{}{appendix.C}{}}
\newlabel{app:voc@cref}{{[appendix][3][2147483647]C}{[1][12][]12}}
\newlabel{fig:topk}{{12}{12}{Evaluation of several attribution methods on Pascal VOC 2012 dataset, using different values of top k\%}{figure.12}{}}
\newlabel{fig:topk@cref}{{[figure][12][2147483647]12}{[1][12][]12}}
\newlabel{fig:more_images}{{13}{13}{Heatmaps of Integrated Gradients (middle) and Geodesic IG (right) on 5 randomly chosen images from the test set of Pascal VOC 2012. We can see that Geodesic IG heatmaps are sharper compared with the original IG method}{figure.13}{}}
\newlabel{fig:more_images@cref}{{[figure][13][2147483647]13}{[1][12][]13}}
\gdef \@abspage@last{13}
