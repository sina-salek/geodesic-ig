\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{marcus2018deep}
\citation{sap2019risk}
\citation{das2020opportunities}
\citation{lundberg2017unified}
\citation{sundararajan2017axiomatic}
\citation{kapishnikov2021guided}
\citation{sundararajan2017axiomatic}
\newlabel{sec:introduction}{{1}{1}{}{section.1}{}}
\newlabel{sec:introduction@cref}{{[section][1][]1}{[1][1][]1}}
\newlabel{fig:duck}{{1}{1}{Comparison of attributions generated by Integrated Gradients (middle image) and Geodesic Integrated Gradients (right image). Integrated Gradients follow straight paths in Euclidean space, which can result in misleading attributions. In contrast, Geodesic Integrated Gradients integrate along geodesic paths on a Riemannian manifold defined by the model, correcting the misattributions due to poor alignment with the model's gradient landscape}{figure.1}{}}
\newlabel{fig:duck@cref}{{[figure][1][]1}{[1][1][]1}}
\newlabel{eq:igi}{{1}{1}{}{equation.1.1}{}}
\newlabel{eq:igi@cref}{{[equation][1][]1}{[1][1][]1}}
\newlabel{fig:ig}{{2}{2}{\textbf {Integrated Gradients (IG) attributions (left) vs. Geodesic IG (right).} The colour map shows feature attributions on the vertical axis, with $(-0.5, -0.5)$ (point B) as the baseline. In IG, points near $A$ are over-attributed due to the straight path crossing high-gradient regions near the decision boundary (gray arrows). Geodesic IG avoids this issue by following geodesic paths, providing more accurate attributions}{figure.2}{}}
\newlabel{fig:ig@cref}{{[figure][2][]2}{[1][2][]2}}
\newlabel{fig:voc_metrics}{{3}{2}{\textbf {Metrics Comparison.} We use a ConNext model for classification of images from the VOC dataset. The horizontal axis represents the top $k\%$ (in absolute value) of features being selected. The top plot, \textit {Comprehensiveness}, shows the average change in the predicted class probability of compared to the original image (higher is better). The bottom plot displays the negative logarithmic probabilities of the predicted class relative to the original (lower is better). In both cases, the results are summarised using AUC, where higher values are better for both}{figure.3}{}}
\newlabel{fig:voc_metrics@cref}{{[figure][3][]3}{[1][2][]2}}
\citation{pascal-voc-2012}
\citation{yang2018geodesic,chen2019fast}
\citation{sundararajan2017axiomatic}
\newlabel{sec:method}{{2}{3}{}{section.2}{}}
\newlabel{sec:method@cref}{{[section][2][]2}{[1][3][]3}}
\newlabel{eq:taylor}{{2}{3}{}{equation.2.2}{}}
\newlabel{eq:taylor@cref}{{[equation][2][]2}{[1][3][]3}}
\newlabel{eq:inner_product}{{3}{3}{}{equation.2.3}{}}
\newlabel{eq:inner_product@cref}{{[equation][3][]3}{[1][3][]3}}
\newlabel{eq:length}{{4}{3}{}{equation.2.4}{}}
\newlabel{eq:length@cref}{{[equation][4][]4}{[1][3][]3}}
\newlabel{rem:shortest}{{2.1}{3}{}{theorem.2.1}{}}
\newlabel{rem:shortest@cref}{{[remark][1][2]2.1}{[1][3][]3}}
\newlabel{eq:loc_geo}{{5}{3}{}{equation.2.5}{}}
\newlabel{eq:loc_geo@cref}{{[equation][5][]5}{[1][3][]3}}
\newlabel{fig:method}{{4}{4}{\textbf {Method overview.} For an input $\textbf {x}$, a baseline $\overline {\textbf {x}}$, and a set of points $\textbf {x}_i$, we compute the $k$NN graph using the euclidean distance (dashed lines). For each couple $(\textbf {x}_i, \textbf {x}_j)$, we then compute the integrated gradients $\textrm {L}^*_{ij}$ using Equation \ref {eq:log_geo_approx}. For clarity, not all $\textrm {L}^*_{ij}$ are present on the figure. 0 and 7 represent $\overline {\textbf {x}}$ and $\textbf {x}$ respectively. Using the resulting undirected weighted graph, we use the Dijkstra algorithm to find the shortest path between $\textbf {x}$ and $\overline {\textbf {x}}$ (blue continuous lines). On the left, the points $\textbf {x}_i$ are provided while, on the right, the points are generated along the straight line between $\textbf {x}$ and $\overline {\textbf {x}}$ (dotted line)}{figure.4}{}}
\newlabel{fig:method@cref}{{[figure][4][]4}{[1][4][]4}}
\newlabel{eq:log_geo_approx}{{6}{4}{}{equation.2.6}{}}
\newlabel{eq:log_geo_approx@cref}{{[equation][6][]6}{[1][3][]4}}
\newlabel{eq:geodesic_ig}{{7}{4}{}{equation.2.7}{}}
\newlabel{eq:geodesic_ig@cref}{{[equation][7][]7}{[1][4][]4}}
\newlabel{eq:deriv_lip}{{8}{4}{}{equation.2.8}{}}
\newlabel{eq:deriv_lip@cref}{{[equation][8][]8}{[1][4][]4}}
\citation{sundararajan2017axiomatic}
\newlabel{fig:bridge}{{5}{5}{When the kNN graph is disconnected, as illustrated here, it would be impossible to compute Geodesic IG between certain points, for instance $\textbf {x}_1$ and $\textbf {x}_5$ here. To solve this, we add a single link between disconnected graphs, here between $\textbf {x}_3$ and $\textbf {x}_7$}{figure.5}{}}
\newlabel{fig:bridge@cref}{{[figure][5][]5}{[1][4][]5}}
\newlabel{eq:energy}{{9}{5}{}{equation.2.9}{}}
\newlabel{eq:energy@cref}{{[equation][9][]9}{[1][5][]5}}
\newlabel{fig:svi_moons}{{6}{5}{\textbf {Visualisation of 10 random paths.} For the simple case of half-moons with 1,000 samples, we display the sampled paths between 10 pairs of points. In low-gradient regions, the sampler favours straight lines, whereas in high-gradient regions, the paths adjust to become nearly perpendicular to the large gradient vectors, crossing these regions as quickly as possible}{figure.6}{}}
\newlabel{fig:svi_moons@cref}{{[figure][6][]6}{[1][5][]5}}
\citation{sundararajan2017axiomatic}
\citation{kapishnikov2021guided}
\citation{scikit-learn}
\citation{jha2020enhanced}
\citation{lundberg2017unified}
\citation{smilkov2017smoothgrad}
\newlabel{eq:norms}{{10}{6}{}{equation.2.10}{}}
\newlabel{eq:norms@cref}{{[equation][10][]10}{[1][6][]6}}
\newlabel{sec:experiments}{{3}{6}{}{section.3}{}}
\newlabel{sec:experiments@cref}{{[section][3][]3}{[1][6][]6}}
\newlabel{subsec:half-moons}{{3.1}{6}{}{subsection.3.1}{}}
\newlabel{subsec:half-moons@cref}{{[subsection][1][3]3.1}{[1][6][]6}}
\newlabel{eq:moons-purity}{{11}{6}{}{equation.3.11}{}}
\newlabel{eq:moons-purity@cref}{{[equation][11][]11}{[1][6][]6}}
\newlabel{eq:moons-std}{{12}{6}{}{equation.3.12}{}}
\newlabel{eq:moons-std@cref}{{[equation][12][]12}{[1][6][]6}}
\citation{pascal-voc-2012}
\citation{liu2022convnet}
\citation{shrikumar2016not}
\citation{lundberg2017unified}
\citation{zeiler2014visualizing}
\citation{tonekaboni2020went}
\citation{kapishnikov2021guided}
\citation{deyoung2019eraser}
\citation{shrikumar2017learning}
\newlabel{tab:results_moons_2}{{1}{7}{Evaluation of different attribution methods on a half-moons dataset with Gaussian noise $\mathcal {N}(0, 0.2)$. The results over 5 different seeds are averaged, with the corresponding standard deviation in brackets. We present in Appendix \ref {app:half-moons} more results with different amounts of Gaussian noise}{table.1}{}}
\newlabel{tab:results_moons_2@cref}{{[table][1][]1}{[1][6][]7}}
\newlabel{subsec:voc}{{3.2}{7}{}{subsection.3.2}{}}
\newlabel{subsec:voc@cref}{{[subsection][2][3]3.2}{[1][7][]7}}
\newlabel{tab:results_voc}{{2}{7}{Evaluation of different attribution methods on 100 randomly sampled images from the Pascal VOC test set. Fig. \ref {fig:voc_metrics} shows the curves where these metrics are extracted from}{table.2}{}}
\newlabel{tab:results_voc@cref}{{[table][2][]2}{[1][7][]7}}
\citation{crane2020survey}
\citation{tenenbaum2000global}
\citation{jha2020enhanced}
\citation{kapishnikov2021guided}
\citation{sundararajan2017axiomatic}
\newlabel{sec:related_work}{{4}{8}{}{section.4}{}}
\newlabel{sec:related_work@cref}{{[section][4][]4}{[1][8][]8}}
\newlabel{fig:enhanced_ig}{{7}{8}{\textbf {Enhanced IG attributions.} Enhanced IG computes a $k$NN algorithm, uses Dijsktra to find the shortest path between an input and a reference baseline, and computes gradients along this path. However, this method is model agnostic and can as a result cross a high gradients region, which is the case in this example, between the input A and the baseline B. Input A therefore has a high attribution which does not reflect the model's true behavior. In this example, the noise is $\mathcal {N}(0, 0.15)$}{figure.7}{}}
\newlabel{fig:enhanced_ig@cref}{{[figure][7][]7}{[1][8][]8}}
\newlabel{sec:discussion}{{5}{8}{}{section.5}{}}
\newlabel{sec:discussion@cref}{{[section][5][]5}{[1][8][]8}}
\citation{langley00}
\bibdata{bib}
\bibcite{chen2019fast}{{1}{2019}{{Chen et~al.}}{{Chen, Ferroni, Klushyn, Paraschos, Bayer, and Smagt}}}
\bibcite{crane2020survey}{{2}{2020}{{Crane et~al.}}{{Crane, Livesu, Puppo, and Qin}}}
\bibcite{das2020opportunities}{{3}{2020}{{Das \& Rad}}{{Das and Rad}}}
\bibcite{deyoung2019eraser}{{4}{2019}{{DeYoung et~al.}}{{DeYoung, Jain, Rajani, Lehman, Xiong, Socher, and Wallace}}}
\bibcite{pascal-voc-2012}{{5}{}{{Everingham et~al.}}{{Everingham, Van~Gool, Williams, Winn, and Zisserman}}}
\bibcite{jha2020enhanced}{{6}{2020}{{Jha et~al.}}{{Jha, K~Aicher, R~Gazzara, Singh, and Barash}}}
\bibcite{kapishnikov2021guided}{{7}{2021}{{Kapishnikov et~al.}}{{Kapishnikov, Venugopalan, Avci, Wedin, Terry, and Bolukbasi}}}
\bibcite{liu2022convnet}{{8}{2022}{{Liu et~al.}}{{Liu, Mao, Wu, Feichtenhofer, Darrell, and Xie}}}
\bibcite{lundberg2017unified}{{9}{2017}{{Lundberg \& Lee}}{{Lundberg and Lee}}}
\bibcite{marcus2018deep}{{10}{2018}{{Marcus}}{{}}}
\bibcite{scikit-learn}{{11}{2011}{{Pedregosa et~al.}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay}}}
\bibcite{sap2019risk}{{12}{2019}{{Sap et~al.}}{{Sap, Card, Gabriel, Choi, and Smith}}}
\bibcite{shrikumar2016not}{{13}{2016}{{Shrikumar et~al.}}{{Shrikumar, Greenside, Shcherbina, and Kundaje}}}
\bibcite{shrikumar2017learning}{{14}{2017}{{Shrikumar et~al.}}{{Shrikumar, Greenside, and Kundaje}}}
\bibcite{smilkov2017smoothgrad}{{15}{2017}{{Smilkov et~al.}}{{Smilkov, Thorat, Kim, Vi{\'e}gas, and Wattenberg}}}
\bibcite{sundararajan2017axiomatic}{{16}{2017}{{Sundararajan et~al.}}{{Sundararajan, Taly, and Yan}}}
\bibcite{tenenbaum2000global}{{17}{2000}{{Tenenbaum et~al.}}{{Tenenbaum, Silva, and Langford}}}
\bibcite{tonekaboni2020went}{{18}{2020}{{Tonekaboni et~al.}}{{Tonekaboni, Joshi, Campbell, Duvenaud, and Goldenberg}}}
\bibcite{yang2018geodesic}{{19}{2018}{{Yang et~al.}}{{Yang, Arvanitidis, Fu, Li, and Hauberg}}}
\bibcite{zeiler2014visualizing}{{20}{2014}{{Zeiler \& Fergus}}{{Zeiler and Fergus}}}
\bibstyle{icml2023}
\newlabel{app:half-moons}{{A}{10}{}{appendix.A}{}}
\newlabel{app:half-moons@cref}{{[appendix][1][2147483647]A}{[1][10][]10}}
\newlabel{fig:noises}{{8}{10}{Evaluation of different attribution methods on the half-moons dataset with different amounts of noise: $\mathcal {N}(0, x)$ where $x$ is defined as the axis of each plot}{figure.8}{}}
\newlabel{fig:noises@cref}{{[figure][8][2147483647]8}{[1][10][]10}}
\newlabel{fig:knn}{{9}{10}{Evaluation of Geodesic IG and Enhanced IG for different values of k in the $k$NN algorithm. We can see that increasing this parameter harms Enhanced IG performance, while it does not seem to have a major effect on Geodesic IG performance}{figure.9}{}}
\newlabel{fig:knn@cref}{{[figure][9][2147483647]9}{[1][10][]10}}
\newlabel{app:voc}{{B}{11}{}{appendix.B}{}}
\newlabel{app:voc@cref}{{[appendix][2][2147483647]B}{[1][11][]11}}
\newlabel{fig:more_images}{{10}{11}{Heatmaps of Integrated Gradients (middle) and Geodesic IG (right) on 5 images from the test set of Pascal VOC 2012}{figure.10}{}}
\newlabel{fig:more_images@cref}{{[figure][10][2147483647]10}{[1][11][]11}}
\gdef \@abspage@last{11}
