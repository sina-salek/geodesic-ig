\section{Experiments}
\label{sec:experiments}

To validate our method, we performed experiments on two datasets: one is the synthetic half-moons dataset, and the other is the real-world Pascal VOC 2012 dataset.

\subsection{Experiments on the half-moons dataset}
\label{subsec:half-moons}

Here we give the details of the half-moons experiment discussed in the Introduction section. We use the half-moons dataset provided by Scikit learn \citep{scikit-learn} to generate 10,000 points with a Gaussian noise of $\mathcal{N}(0, 0.2)$. The dataset is split into 8,000 training points and 2,000 testing ones. The model used is an MLP.

We measure two indicators of performance for each attribution method: a lack of artifacts not reflecting the model's behaviour, and a low variation of attributions between close points. To that goal, we use two metrics, \textbf{purity} and \textbf{standard deviation}, defined in the following way. We know that a well-trained model should classify about half of the data points as upper moon, class $1$, and the other half as lower moon, class $0$. Such a model should consider both features of each point important for the classification into class $1$. Therefore, for such a model, we expect in a good attribution method, the top $50\%$ points as ranked by the quantity $\widetilde{attr}(\textbf{x}; f) = \sum_{i=0}^1 |attr_i(\textbf{x}; f)|$, to be classified as $1$ (assuming the baseline is chosen as a point to which the network gives a near-zero score). With this in mind, purity is defined as
\begin{equation}
\begin{split}
    \textrm{Purity} &= \frac{1}{N/2}\sum_{\textbf{x}, \, \widetilde{attr}(\textbf{x}; f) \in \textrm{Top 50\% of all attr}} \textrm{argmax}(f(\textbf{x})),
\end{split}
\label{eq:moons-purity}
\end{equation}
where $N$ is the number of data points. We see that this is the average value of the predicted class labels for half of the points. From the above, we infer that, for a well-trained model, we prefer an attribution method that results in the purity close to $1$. In contrast, a random attribution method in this case would result in the purity score of $0.5$. 

For the second metric, standard deviation, points that belong to the same moon to have similar $\widetilde{attr}(\textbf{x}; f)$. Therefore, define
\begin{equation}
\begin{split}
    \textrm{Std}_i &= \textrm{std}\{\widetilde{attr}(\textbf{x}; f), \, \textrm{argmax}(f(\textbf{x})) \in \textrm{Moon }_i\}.
\end{split}
\label{eq:moons-std}
\end{equation}
We expect this standard deviation metric to be low for the points belonging to either class. 

In this experiment, we compare the results of attributions from Geodesic IG with the original IG, as well as more recent comparable methods including Enhanced IG \citep{jha2020enhanced}, GradientShap \citep{lundberg2017unified}, SmoothGrad \citep{smilkov2017smoothgrad}.

For all of the methods, we use $(-0.5, -0.5)$ as a baseline. Enhanced IG is an improvement over IG where the kNN algorithm is used to avoid computing gradients on paths on out of sample distributions. The chosen number of neighbours for the kNN part of both Enhanced IG and Geodesic IG is $5$. We perform an ablation study of this parameter in Appendix \ref{app:half-moons}. 

\begin{table}[t]
	\centering
	\resizebox{0.48\textwidth}{!}{%
	\begin{tabular}{lccc}
		\toprule
		\textbf{Method} & Purity $\uparrow$ & $\textrm{STD}_0$ $\downarrow$ & $\textrm{STD}_1$ $\downarrow$ \\
		\midrule
        GradientShap    & 0.761  & 1.90   & 5.01 \\
		IG              & 0.802  & 1.44 & 4.915  \\
        SmoothGrad      & 0.800   &  1.42  & 4.720  \\
        Enhanced IG     & 0.738   &  1.23  & 1.876 \\
		\midrule
		Geodesic IG     & \textbf{0.978} & \textbf{0.237} & \textbf{0.267} \\
		\bottomrule
	\end{tabular}%
	}
	\caption{Evaluation of different attribution methods on a half-moons dataset with Gaussian noise $\mathcal{N}(0, 0.2)$. The results over 5 different seeds are averaged, with the corresponding standard deviation in brackets. We present in Appendix \ref{app:half-moons} more results with different amounts of Gaussian noise.}
	\label{tab:results_moons_2}
\end{table}

The quantitative analysis of the results are given in Table \ref{tab:results_moons_2}, where we see that Geodesic IG significantly outperforms all other methods on all metrics. In particular, notice that all other methods perform relatively poorly on STD$_1$. This is because for this metric, the integration path has to cross the decision boundary. However, the other methods might cross the decision boundary differently and regardless of the function's gradient, resulting in spuriously different attributions for different points belonging to class $1$. In Appendix \ref{app:half-moons} we see that the gap between the performance of geodesic IG and the other methods increases as the Gaussian noise of the half-moons increases. To provide better understanding of these results we present more analysis on this dataset in Section \ref{sec:related_work}.

\subsection{Experiments on the Pascal VOC 2012 dataset}
\label{subsec:voc}

Now we would like to test our method on a real-world dataset. To this aim, we use the Pascal VOC 2012 dataset, which consists of labelled images \citep{pascal-voc-2012}. We trained a classification head on this dataset and added it to the pre-trained ConvNext model from torch vision to generate predictions to be explained \citep{he2016deep}. We also perform the analysis on 100 randomly sampled images from the test set.

We compare our method with various baselines: Integrated Gradients, GradientShap, InputXGradients \citep{shrikumar2016not}, KernelShap \citep{lundberg2017unified}, Occlusion \citep{zeiler2014visualizing} Augmented Occlusion \citep{tonekaboni2020went} and Guided IG \citep{kapishnikov2021guided}. 

Where an explainer needs a baseline, such as in IG as well as our method, a uniformly black image was chosen as baseline.

To evaluate the performance of an attribution method, we use 2 different metrics:

\begin{itemize}
    \item \textbf{Comprehensiveness} \citep{deyoung2019eraser}: We mask the top k\% most important features in absolute value, and compute the average change of the predicted class probability compared with the original image. A higher score is better as it indicates masking these features results in a large change of predictions.

    \item \textbf{Log-odds}\footnote{This metric should be called \emph{Log-probabilities}. However, since Log-odds is a commonly used name in the literature, we refer to it as Log-odds.} \citep{shrikumar2017learning}: We mask the top k\% most important features in absolute value, and measure the negative logarithmic probabilities on the predicted class compared with the original one. Lower scores are better.
\end{itemize}

\begin{table}[t]
	\centering
	\resizebox{0.48\textwidth}{!}{%
	\begin{tabular}{lccc}
		\toprule
		\textbf{Method} & AUC-Comp $\uparrow$  & AUC-LO $\uparrow$ \\
		\midrule
        Input X Gradients   & 0.21 & 1.28  \\
        GradientShap        & 0.18  & 1.15  \\
		IG                  &0.21  & 1.25  \\
        Random                & 0.16 & 0.86  \\
        Kernel Shap         & 0.16 & 0.94 \\
        Occlusion           & 0.19 &  1.16 \\
        Aug Occlusion       &0.19 & 1.15\\
        Guided IG         & 0.20 & 1.18 \\
		\midrule
		Geodesic IG         & \textbf{0.27}  & \textbf{1.44}  \\
		\bottomrule
	\end{tabular}%
	}
	\caption{Evaluation of different attribution methods on 100 randomly sampled images from the Pascal VOC test set. Fig. \ref{fig:voc_metrics} shows the curves where these metrics are extracted from.}
	\label{tab:results_voc}
\end{table}

We calculated these two metrics for a range of top-$k$\%s, from 1\% to 56\%, which is shown in Fig. \ref{fig:voc_metrics}. To summarise the values from different top-$k$\%s, we calculate the area between the curves and the horizontal line at $y = 0$, a.k.a. area under the curve, and report them in Table \ref{tab:results_voc}.   Fig. \ref{fig:puppy} provides a qualitative comparison between Geodesic IG and Integrated Gradients. The analysis shows that Geodesic IG outperforms other methods in explaining the model's behaviour on the dataset, and it does so with a striking gap in the case of comprehensiveness.

The one downside of using Geodesic IG for explaining complex deep learning models is the computational cost. Running the above experiment on 100 images took 23 hours using an L4 GPU. However, this would be a reasonable price to pay to get more accurate explanations in use-cases where understanding the model is more critical.